{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook: Flood Detection with Sen1Floods11 Dataset on SharingHub\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook demonstrates how to organize, train, and use a deep learning model for image segmentation through the flood detection using the [**Sen1Floods11 dataset**](https://github.com/cloudtostreet/Sen1Floods11?tab=readme-ov-file), leveraging the [**SharingHub**](https://sharinghub.p2.csgroup.space/#/) platform.\n",
    "\n",
    "### Key Features of this Demo:\n",
    "1. **Dataset Management**: Retrieve and version datasets stored on SharingHub, using DVC and GitLab integration.\n",
    "2. **Experiment Tracking**: Configure and use MLflow for tracking training experiments, directly linked to the GitLab repository.\n",
    "3. **Model Training**: Train the flood detection model with data streamed from SharingHub’s STAC API or its DVC remote.\n",
    "4. **Model Inference**: Use the trained model to perform segmentation on unseen data, predicting flooded areas.\n",
    "5. **Automation**: Streamline operations with Docker containers and CWL workflows for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "### 1. **Training the Model**\n",
    "- Store the model in onnx format.\n",
    "- Get metrics to evaluate the model performances.\n",
    "- Track the training progress using MLflow, with runs linked to the model’s GitLab repository.\n",
    "\n",
    "### 2. **Inference**\n",
    "- Perform segmentation on Sentinel-1 images to detect flooded areas.\n",
    "- Visualize results to evaluate model performance.\n",
    "\n",
    "### 3. **Reproducibility**\n",
    "- Use Docker containers and CWL workflows for consistent environment setup and execution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prerequisites\n",
    "\n",
    "Before entering the demonstration, make sure you have followed the tutorial in the [readme](../README.md) to be sure you have correctly configured your environment as well as your environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run python3 src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry run python3 src/inference.py checkpoints/Sen1Floods11_0_0.5194225907325745.onnx inference/India_80221_S1Hand.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Reproducibility**\n",
    "## **Docker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker image for training from Dockerfile.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your credentials in a .env file and run the docker image:\n",
    "-   MLFLOW_TRACKING_TOKEN=\n",
    "-   LOGNAME=\n",
    "-    ACCESS_KEY_ID= \n",
    "-    SECRET_ACCESS_KEY= \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile.train -t train ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Mode in `train`\n",
    "\n",
    "The *streaming* mode in `train` allows data to be downloaded on the fly, image by image, during training. Then the downloaded data is stored in the cache. This eliminates the need to download the **entire** dataset, saving both time and storage space. Data is progressively loaded from remote storage via DVC, reducing memory usage and optimizing the training process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Cache Mode in `train`\n",
    "\n",
    "If *no cache* mode in `train` is enabled with the *streaming mode*, the data is downloaded on the fly, but not saved locally in the cache. This mode is ideal for handling large datasets without overwhelming local resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --env-file .env train bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker image for inference from Dockerfile.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile.inf -t inference ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the docker image for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run inference checkpoints/Sen1Floods11_0_0.5194225907325745.onnx inference/India_80221_S1Hand.tif && ls predictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CWL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the docker image using cwl with custom parameters saved in run_inference_input.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cwltool run_inference.cwl run_inference_input.yml "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sen1floods11-isRT4Uh--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
