{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook: Flood Detection with Sen1Floods11 Dataset on SharingHub\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook demonstrates how to organize, train, and use a deep learning model for image segmentation through the flood detection using the [**Sen1Floods11 dataset**](https://github.com/cloudtostreet/Sen1Floods11?tab=readme-ov-file), leveraging the [**SharingHub**](https://sharinghub.p2.csgroup.space/#/) platform.\n",
    "\n",
    "### Key Features of this Demo:\n",
    "1. **Dataset Management**: Retrieve and version datasets stored on SharingHub, using DVC and GitLab integration.\n",
    "2. **Experiment Tracking**: Configure and use MLflow for tracking training experiments, directly linked to the GitLab repository.\n",
    "3. **Model Training**: Train the flood detection model with data streamed from SharingHub‚Äôs STAC API or its DVC remote.\n",
    "4. **Model Inference**: Use the trained model to perform segmentation on unseen data, predicting flooded areas.\n",
    "5. **Automation**: Streamline operations with Docker containers and CWL workflows for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "### 1. **Training the Model**\n",
    "- Store the model in onnx format.\n",
    "- Get metrics to evaluate the model performances.\n",
    "- Track the training progress using MLflow, with runs linked to the model‚Äôs GitLab repository.\n",
    "\n",
    "### 2. **Inference**\n",
    "- Perform segmentation on Sentinel-1 images to detect flooded areas.\n",
    "- Visualize results to evaluate model performance.\n",
    "\n",
    "### 3. **Reproducibility**\n",
    "- Use Docker containers and CWL workflows for consistent environment setup and execution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prerequisites\n",
    "\n",
    "Before entering the demonstration, make sure you have followed the tutorial in the [readme](../README.md) to be sure you have correctly configured your environment as well as your environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING MODE DISABLED.\n",
      "sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/S1Hand/Ghana_103272_S1Hand.tif sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/LabelHand/Ghana_103272_LabelHand.tif\n",
      "sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/S1Hand/USA_831672_S1Hand.tif sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/LabelHand/USA_831672_LabelHand.tif\n",
      "sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/S1Hand/Pakistan_246510_S1Hand.tif sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/LabelHand/Pakistan_246510_LabelHand.tif\n",
      "sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/S1Hand/Ghana_5079_S1Hand.tif sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/LabelHand/Ghana_5079_LabelHand.tif\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:41<00:00,  6.34s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:40<00:00,  6.28s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:40<00:00,  6.29s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:41<00:00,  6.34s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:41<00:00,  6.32s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:40<00:00,  6.27s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:47<00:00,  6.69s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:48<00:00,  6.81s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [02:08<00:00,  8.00s/it]\n",
      "Training Loop: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:42<00:00,  6.39s/it]\n",
      "Current Epoch: 0\n",
      "model saved at checkpoints/Sen1Floods11_0_0.5324235558509827.onnx .\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:15<00:00,  2.00s/it]\n",
      "model saved in MLflow.\n",
      "Training Loss: tensor(0.3631, grad_fn=<DivBackward0>)\n",
      "Training IOU: tensor(0.4041)\n",
      "Training Accuracy: tensor(0.8793)\n",
      "Validation Loss: tensor(0.3728)\n",
      "Validation IOU: tensor(0.5324)\n",
      "Validation Accuracy: tensor(0.9454)\n",
      "max valid iou: tensor(0.5324)\n",
      "2024/12/17 11:06:05 INFO mlflow.tracking._tracking_service.client: üèÉ View run traveling-duck-525 at: https://sharinghub.p2.csgroup.space/mlflow/space_applications/mlops-services/sample-projects/ai-models/flood-model/tracking/#/experiments/23/runs/123ec611780f47eba2aa7814ccdaefc5.\n",
      "2024/12/17 11:06:05 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://sharinghub.p2.csgroup.space/mlflow/space_applications/mlops-services/sample-projects/ai-models/flood-model/tracking/#/experiments/23.\n"
     ]
    }
   ],
   "source": [
    "!poetry run python3 src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image saved at predictions/prediction.tif\n"
     ]
    }
   ],
   "source": [
    "!poetry run python3 src/inference.py checkpoints/Sen1Floods11_0_0.5194225907325745.onnx sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/S1Hand/India_80221_S1Hand.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Reproducibility**\n",
    "## **Docker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker image for training from Dockerfile.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your credentials in a .env file and run the docker image:\n",
    "-   MLFLOW_TRACKING_TOKEN=\n",
    "-   LOGNAME=\n",
    "-    ACCESS_KEY_ID= \n",
    "-    SECRET_ACCESS_KEY= \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                          docker:default\n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (1/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        0.4s\n",
      " => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        0.5s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        0.7s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        0.8s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.10-slim        1.0s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (3/3)                                          docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10-slim        1.0s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (11/12)                                        docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10-slim        1.0s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 184B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.10-slim@sha256:61912260e578182d  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.22kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] COPY requirements_train.txt /app/requirements_train.txt   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] RUN apt-get update &&     apt-get install -y libexpat1 g  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY src /app/src                                         0.0s\n",
      "\u001b[0m\u001b[34m => [5/6] COPY sen1floods11-dataset /app/sen1floods11-dataset              0.0s\n",
      "\u001b[0m\u001b[34m => [6/6] WORKDIR /app                                                     0.1s\n",
      "\u001b[0m => exporting to image                                                     0.0s\n",
      "\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (12/12) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load build definition from Dockerfile.train                 0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 572B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.10-slim        1.0s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 184B                                          0.0s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.10-slim@sha256:61912260e578182d  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 6.22kB                                        0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] COPY requirements_train.txt /app/requirements_train.txt   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] RUN apt-get update &&     apt-get install -y libexpat1 g  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] COPY src /app/src                                         0.0s\n",
      "\u001b[0m\u001b[34m => [5/6] COPY sen1floods11-dataset /app/sen1floods11-dataset              0.0s\n",
      "\u001b[0m\u001b[34m => [6/6] WORKDIR /app                                                     0.1s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:98d744782e057e2e72ff87dc35e6adddcd9955a022148  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/train                                   0.0s\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -f Dockerfile.train -t train ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Mode in `train`\n",
    "\n",
    "The *streaming* mode in `train` allows data to be fetched on-the-fly, image by image, during training. This eliminates the need to download the entire dataset, saving both time and storage space. Data is progressively loaded from remote storage via DVC, reducing memory usage and optimizing the training process. This mode is ideal for handling large datasets without overwhelming local resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMING MODE ENABLED.\n",
      "CACHE IS ENABLED.\n",
      "Training Loop: 16it [04:49, 18.12s/it]\n",
      "Training Loop: 16it [01:15,  4.72s/it]\n",
      "Training Loop: 16it [01:16,  4.77s/it]\n",
      "Training Loop: 16it [01:16,  4.81s/it]\n",
      "Training Loop: 16it [01:19,  4.95s/it]\n",
      "Training Loop: 16it [01:30,  5.68s/it]\n",
      "Training Loop: 16it [01:33,  5.86s/it]\n",
      "Training Loop: 16it [01:33,  5.86s/it]\n",
      "Training Loop: 16it [01:25,  5.32s/it]\n",
      "Training Loop: 16it [01:24,  5.28s/it]\n",
      "Current Epoch: 0\n",
      "model saved at checkpoints/Sen1Floods11_0_0.17997555434703827.onnx .\n",
      "model saved in MLflow.\n",
      "Training Loss: tensor(0.3634, grad_fn=<DivBackward0>)\n",
      "Training IOU: tensor(0.3858)\n",
      "Training Accuracy: tensor(0.8839)\n",
      "Validation Loss: tensor(0.6157)\n",
      "Validation IOU: tensor(0.1800)\n",
      "Validation Accuracy: tensor(0.9158)\n",
      "max valid iou: tensor(0.1800)\n",
      "üèÉ View run unequaled-finch-393 at: https://sharinghub.p2.csgroup.space/mlflow/space_applications/mlops-services/sample-projects/ai-models/flood-model/tracking/#/experiments/23/runs/75607d930ef8470daa319392c4d55c28\n",
      "üß™ View experiment at: https://sharinghub.p2.csgroup.space/mlflow/space_applications/mlops-services/sample-projects/ai-models/flood-model/tracking/#/experiments/23\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!docker run -it --env-file .env train bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker image for inference from Dockerfile.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f Dockerfile.inf -t inference ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the docker image for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run inference checkpoints/Sen1Floods11_1_0.4923180341720581.onnx sen1floods11-dataset/v1.1/data/flood_events/HandLabeled/S1Hand/India_80221_S1Hand.tif && ls predictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CWL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the docker image using cwl with custom parameters saved in run_inference_input.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cwltool --debug run_inference.cwl run_inference_input.yml "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sen1floods11-isRT4Uh--py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
